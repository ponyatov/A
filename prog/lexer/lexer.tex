\secrel{Лексический и синтаксический анализ}\secdown

Очень часто в практике возникает необходимость работы с данными в текстовых
форматах\ --- \termdef{plain text}{plain text} файлы, в которых в каком-либо
формате (на языке разметки, или \termdef{DDL}{DDL}: \termdef{[D]ata
[D]efinition [L]anguage}{Data Definition Language}) описаны данные. И от вас
требуется реализовать разбор такого файла, выделяя синтаксические структуры и
элементы данных, чтобы в дальнейшем после их преобразования например записать
текстовый файл в другом формате.

В таком виде хранятся результаты расчетных программ, работащих в пакетном
режиме, данные с измерительных систем, поток данных с приемников
GPS\note{протокол NMEA 0183}, очень популярный мета-формат XML со всеми его
частными случаями типа HTML, XLIFF\ref{xliff}, OpenDocument, тексты программ
для станков с ЧПУ,\ldots

С некоторыми хинтами точно так же можно работать и с бинарными файлами,
преобразовав их сначала в текстовую форму (в простейшем случае просто сделав hex
dump).

В некоторых случаях необходимо написание трансляторов форматов (текстовых)
данных, или даже интерпретаторов/компиляторов языков программирования.

\bigskip
Все эти техники с использованием стандартных утилит \prog{flex}\ и \prog{bison}\
будут кратко описаны в этой главе. Подробнее эти техники рассмотрены в
книгах\ref{lexlit}, особенно стоит отметить талмуд
\textbf{DragonBook}:

\bigskip

\label{exdragon}\cite{dragonbook} \textbf{Книга Дракона}: Ахо, Сети, Ульман
Принципы построения компиляторов.

\bigskip

\href{http://habrahabr.ru/post/99162/}{Habr: Компиляция. 1: лексер}

\href{http://habrahabr.ru/post/99298/}{Habr: Компиляция. 2: грамматики}

\href{http://habrahabr.ru/post/99366/}{Habr: Компиляция. 3: бизон}

\href{http://habrahabr.ru/post/99397/}{Habr: Компиляция. 4: игрушечный ЯП}

\href{http://habrahabr.ru/post/99466/}{Habr: Компиляция. 5: нисходящий разбор}

\href{http://habrahabr.ru/post/102597/}{Habr: Компиляция. $5\frac{1}{2}$: llvm
как back-end}

\href{http://habrahabr.ru/post/99592/}{Habr: Компиляция. 6: промежуточный код}


\secrel{Лексер и лексический анализ, утилита \prog{flex}}

\begin{framed}
\noindent
\termdef{Л\'{е}ксер}{лексер}/\termdef{сканер}{сканер}\ --- программа или ее
часть, которая \begin{enumerate}[nosep]
  \item
получает на вход исходные
данные в виде сплошного потока одиночных сиволов,
  \item
группирует символы согласно
набору правил (заданных \termdef{регулярными выражениями}{регулярное
выражение}) и
  \item
отдает на выходе символы, уже сгруппированные в \termdef{лекс\'{е}мы}{лексема}\
или \termdef{ток\'{е}ны}{токен}.
\end{enumerate}
Цель лексера\ --- подготовить последовательность лексем для входа другой
программы. \\
В самый простых случаях на лексер можно возложить простые преобразования текста.
\end{framed}

\termdef{Лексический анализ}{лексический анализ}\ --- процесс программного
разбора входной последовательности символов\note{например, такой как исходный
код на одном из языков программирования} с целью получения на выходе
последовательности групп символов\ --- \term{токенов}, имеющих собственное
смысловое значение\note{подобно группировке букв в слово}. Как правило,
лексический анализ производится в соответствии набора правил определённого
\term{формального, искуственного или компьютерного языка}.

\begin{framed}\noindent
\term{Компьютерный язык}, а точнее его \termdef{грамматика}{грамматика}, задаёт
определённый набор лексем, которые могут встретиться на входе лексера, и набор
правил, по которым их следует группировать.
\end{framed}

Традиционно принято организовывать процесс лексического анализа, рассматривая
входную последовательность символов как поток одиночных символов. При такой
организации \term{лексер}\ самостоятельно управляет выборкой отдельных символов
из входного потока.

Распознавание лексем с учетом грамматики обычно производится путём их
идентификации согласно идентификаторам токенов, определяемых грамматикой языка.
При этом любая последовательность символов входного потока (лекс\'{е}ма),
которая согласно грамматике не может быть идентифицирована как токен языка,
обычно рассматривается как специальный \term{токен-ошибка}.

\bigskip
Каждый выделенный токен можно представить в виде парной структуры,
содержащей
\begin{enumerate}
  \item идентификатор токена и
  \item саму последовательность символов лексемы, выделенной из входного
потока\note{запись строки, числа и т. д.}.
\end{enumerate}

\bigskip
Рассморим обработку текстового файла: разделение текстового фрагмента на абзацы,
запись в формате
\href{http://docs.oasis-open.org/xliff/xliff-core/xliff-core.html}{XLIFF} для
перевода в системе \href{http://smartcat.pro}{ABBYY SmartCAT}, и обратной
трансляции из XLIFF в \LaTeX-совместимое форматирование.

\bigskip
Так как задача построения \termdef{лексических анализаторов}{лексический
анализатор} является стандартной задачей информатики, был разработан типовой
инструмент: \term{генератор лексических анализаторов} \prog{flex}. Эта программа
транслирует описание лексера на своем высококоуровневом языке в фрагмент
программы на языке Си/\cpp\ или самостоятельную программу. Описание лексера
прописывается в \file{.lex}-файле в формате:

\begin{verbatim}
определения, опции, декларации
%%
правила выделения токенов через регулярки
%%
сишный код
\end{verbatim}

\paragraph{Комментарии} поддерживаются сишные \verb|/* */| комментарии.

\paragraph{Опции} \verb|%option|

\bigskip\noindent
\begin{tabular}{l l}
noyywrap & отключает вызов лексером функции \verb|yywrap()| при достижении
конца текущего файла \\
main & включение типовой функции \verb|main()| вместо заданной пользователем \\
case-insensitive & регистро-независимый лексический анализ, большие/маленькие
буквы не различаются \\
yylineno & в глобальной си-переменной \verb|yylineno| доступен номер
текущей строки \\
\end{tabular}

\paragraph{Формат опреления} \verb|имя определение|:
\begin{verbatim}
digit    [0-9]
number   [\+\-]{0,1}{digit}+\.{digit}*
\end{verbatim}

\paragraph{Декларации на Си} прописываются в скобках \verb|%{ }%|

\cp{http://matt.might.net/articles/standalone-lexers-with-lex/}

\lst{xliff/txt/txt2xliff.lex}{}{xliff/txt/txt2xliff.lex}

\secrel{Генератор синтаксических анализаторов \prog{bison}}

\begin{framed}
\termdef{Синтаксический анализ}{синтаксический анализ}\ --- процесс анализа
последовательности токенов с определением их грамматической структуры. На этом
этапе выделяются \term{синтаксичекие ошибки}.
\end{framed}

\secrel{Семантический анализ}

\begin{framed}
\termdef{Семантический анализ}{семантический анализ}\ --- процесс выполнения
\term{семантических проверок}: контроль типов, привязка объектов и т.д. 
\end{framed}

\secrel{Оптимизация}

Выполнение формальных преобразований структур данных, описываютщих
компилируюемую программу, с целью построения более компактного/быстрого
машинного кода.

\secrel{Кодогенерация}

Получение реального машинного кода в бинарном представлении, или в виде
ассемблерных текстовых файлов.

\secrel{Дополнительная литература}\label{lexlit}

\bigskip

\url{http://alumni.cs.ucr.edu/~lgao/teaching/flex.html}

\url{http://www.capsl.udel.edu/courses/cpeg421/2012/slides/Tutorial-Flex\_Bison.pdf}

\secrel{Транслятор Паскаля}

\secrel{LLVM и разработка собственных компиляторов}

\secup
