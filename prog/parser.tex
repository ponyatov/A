\chapter{Лексический и синтаксический анализ}

Очень часто в практике возникает необходимость работы с данными в текстовых
форматах\ --- plain text файлы, в которых в каком-либо формате\note{на языке
разметки, или DDL [D]ata [D]efinition [L]anguage}\ описаны данные. И от вас
требуется реализовать разбор такого файла, выделяя элементы данных, чтобы в
дальнейшем после их преобразования например записать текстовый файл в другом
формате.

В таком виде хранятся результаты расчетных программ, работащих в пакетном
режиме, данные с измерительных систем, поток данных с приемников GPS\note{протокол
NMEA 0183}, очень популярный мета-формат XML со всеми его частными случаями типа
HTML, XLIFF\ref{xliff}, OpenDocument, тексты программ для станков с ЧПУ,\ldots

С некоторыми хинтами точно так же можно работать и с бинарными файлами,
преобразовав их сначала в текстовую форму (в простейшем случае просто сделав hex
dump).

В некоторых случаях необходимо написание трансляторов форматов (текстовых)
данных, или даже интерпретаторов/компиляторов языков программирования.

Все эти техники будут кратко описаны в этой главе. Подробнее эти техники
рассмотрены в книгах, особенно стоит отметить талмуд
\textbf{DragonBook} \cite{dragonbook}.

\begin{framed}
\term{Лексический анализ}\ --- процесс программного разбора входной
последовательности символов\note{например, такой как исходный код на одном из
языков программирования} с целью получения на выходе последовательности
символов, называемых «\term{токенами}»\note{подобно группировке букв в слово}.

Группа символов входной последовательности, идентифицируемая на выходе процесса как
\term{токен}, также называется \term{лексемой}. 
\end{framed}

Как правило, лексический анализ производится с учетом определённого
\term{формального языка}\ или набора языков. 

\begin{framed}
\term{Язык}, а точнее его
\term{грамматика}, задаёт определённый набор лексем, которые могут встретиться
на входе процесса.
\end{framed}

Традиционно принято организовывать процесс лексического анализа, рассматривая
входную последовательность символов как поток одиночных символов. При такой
организации \term{лексер}\ самостоятельно управляет выборкой отдельных символов
из входного потока.

Распознавание лексем с учетом грамматики обычно производится путём их
идентификации согласно идентификаторам токенов, определяемых грамматикой языка.
При этом любая последовательность символов входного потока (лексема), которая
согласно грамматике не может быть идентифицирована как токен языка, обычно
рассматривается как специальный \term{токен-ошибка}.

Каждый токен можно представить в виде парной структуры, содержащей
\begin{enumerate}
  \item идентификатор токена и
  \item саму последовательность символов лексемы, выделенной из входного
потока\note{запись строки, числа и т. д.}.
\end{enumerate}

Цель лексера\ --- подготовить входную последовательность для другой программы.

\begin{framed}
Переформулируя: \term{лексер}\ --- программа, которая получает на вход исходные
данные в виде сплошного потока одиночных сиволов, группирует символы согласно
набору правил, и отдает на выходе сиволы уже сгруппированные в \term{лексемы}. 
\end{framed}

\section{Применение flex/bison для разбора текстовых форматов данных}

\href{http://habrahabr.ru/post/99162/}{Habr: Компиляция. 1: лексер}

\href{http://habrahabr.ru/post/99298/}{Habr: Компиляция. 2: грамматики}

\href{http://habrahabr.ru/post/99366/}{Habr: Компиляция. 3: бизон}

\href{http://habrahabr.ru/post/99397/}{Habr: Компиляция. 4: игрушечный ЯП}

\href{http://habrahabr.ru/post/99466/}{Habr: Компиляция. 5: нисходящий разбор}

\href{http://habrahabr.ru/post/102597/}{Habr: Компиляция. $5\frac{1}{2}$: llvm
как back-end}

\href{http://habrahabr.ru/post/99592/}{Habr: Компиляция. 6: промежуточный код}


\section{Компилятор Паскаля}
